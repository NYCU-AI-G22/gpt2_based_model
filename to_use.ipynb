{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GvWI_sfST2_",
        "outputId": "6d398a8b-f4fd-42c5-eee6-5f88fc6ec31f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "pDWqQAK4Sde3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir(save_directory))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3ogI_jlTKpP",
        "outputId": "77b979d2-9b7c-4ef2-a786-bb7004066327"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['config.json', 'training_args.bin', 'generation_config.json', 'model.safetensors']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OA4XdYqCSCZ0",
        "outputId": "9488550d-95bb-485b-c796-e8e8d53df676"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The weather is bad today. The wind blows really fast today, sucks today \n"
          ]
        }
      ],
      "source": [
        "# Define the path to the saved model\n",
        "save_directory = \"/content/drive/MyDrive/112-2 ai final project/saved_model\"\n",
        "\n",
        "# Load the tokenizer and model from the saved directory\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
        "model = AutoModelForCausalLM.from_pretrained(save_directory)\n",
        "\n",
        "# Create the text generation pipeline\n",
        "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Generate a response\n",
        "input_text = \"The weather is bad today.\"\n",
        "response = generator(input_text, max_length=50)\n",
        "print(response[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G5KaJ822TfBA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}